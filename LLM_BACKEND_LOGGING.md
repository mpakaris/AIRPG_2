# LLM Backend Logging - Reference Guide

## âœ… What Was Added

### 1. Main Game Flow (Safety Net)
**Location:** `src/app/actions.ts`

Shows which LLM backend is being used for command interpretation:

```
ğŸ¤– â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   AI COMMAND INTERPRETATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“¥ Player Input: "look around"
ğŸ¯ Command Output: "look around"
ğŸ“Š Confidence: 75.0%
ğŸ”„ AI Calls: 1 (primary)
ğŸŒ LLM Backend: â˜ï¸  API (Gemini Flash Lite)  â† NEW!
â±ï¸  Latency: 342ms
   â”œâ”€ Primary AI: 75.0% (Gemini)
   â””â”€ Safety AI: (not called)
ğŸ’­ Reasoning: Player wants to survey the area
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Current Status:** Will always show `â˜ï¸ API (Gemini Flash Lite)` because your game uses the Safety Net flow.

---

### 2. Local LLM Hybrid Flow (When Used)
**Location:** `src/ai/flows/interpret-player-commands-hybrid.ts`

**If Local LLM is Used:**
```
ğŸ¤– USING LOCAL LLM (Ollama/Docker) - Request #1
ğŸŒ LLM Backend: ğŸ–¥ï¸  LOCAL (Llama 3.2 3B)  â† Shows local!
ğŸ“ Command: "look around"
âœ… Local LLM responded in 234ms
ğŸ’° Cost: $0.00 (FREE!)
ğŸ“Š Session Stats: Local=1 | API=0
```

**If Falls Back to API:**
```
âš ï¸  LOCAL LLM HEALTH CHECK FAILED - Falling back to API
ğŸŒ LLM Backend: â˜ï¸  API (Gemini Flash Lite) - FALLBACK
ğŸ“Š Session Stats: Local=0 | API=1
```

**If API Mode Configured:**
```
â˜ï¸  USING API LLM (Gemini) - Request #1
ğŸŒ LLM Backend: â˜ï¸  API (Gemini Flash Lite)
ğŸ“ Command: "look around"
ğŸ“Š Session Stats: Local=0 | API=1
```

---

## ğŸ“Š LLM Backend Indicators

| Icon | Backend | Model | Cost | Requires Internet |
|------|---------|-------|------|-------------------|
| ğŸ–¥ï¸ | LOCAL | Llama 3.2 3B (Ollama) | $0.00 | âŒ No |
| â˜ï¸ | API | Gemini Flash Lite | ~$0.001/cmd | âœ… Yes |
| â˜ï¸ | API (FALLBACK) | Gemini Flash Lite | ~$0.001/cmd | âœ… Yes |

---

## ğŸ® What You'll See When Playing

### Current Setup (Safety Net with API)
Every command will show:
```
ğŸŒ LLM Backend: â˜ï¸  API (Gemini Flash Lite)
```

This is expected! Your game uses the Safety Net flow which is API-based.

---

### If You Switch to Local LLM (Future)
If we integrate local LLM into the Safety Net flow, you'd see:
```
ğŸŒ LLM Backend: ğŸ–¥ï¸  LOCAL (Llama 3.2 3B)
```

And the session stats would track:
```
ğŸ“Š Session Stats: Local=25 | API=0
```

Showing you're using 100% local LLM!

---

## ğŸ” Quick Reference: Which Flow Uses What?

### Current Game Flow
```
Player Input
    â†“
interpretCommandWithSafetyNet (actions.ts)
    â†“
guidePlayerWithNarrator (Gemini Flash API) â† Always API
    â†“
Command Execution
```

**Log shows:** `ğŸŒ LLM Backend: â˜ï¸  API (Gemini Flash Lite)`

---

### Local LLM Hybrid Flow (Not Currently Used)
```
Player Input
    â†“
interpretPlayerCommandHybrid
    â†“
Check NEXT_PUBLIC_LLM_MODE
    â†“
â”œâ”€ "local" â†’ interpretPlayerCommandLocal (Ollama) â†’ ğŸ–¥ï¸  LOCAL
â””â”€ "api"   â†’ interpretPlayerCommandAPI (Gemini)   â†’ â˜ï¸  API
```

**Log shows:** Either `ğŸ–¥ï¸ LOCAL` or `â˜ï¸ API` depending on mode

---

## ğŸ§ª Testing

### Test Current Game (API)
```bash
npm run dev
```
Enter "look around"

**Expected log:**
```
ğŸŒ LLM Backend: â˜ï¸  API (Gemini Flash Lite)
```

### Test Local LLM (Standalone)
```bash
npm run test:llm
```

**Expected log:**
```
ğŸŒ LLM Backend: ğŸ–¥ï¸  LOCAL (Llama 3.2 3B)
```
(But only if Ollama is running and NEXT_PUBLIC_LLM_MODE=local)

---

## ğŸ’¡ Want to See Local LLM in Your Game?

To integrate local LLM into your game's Safety Net flow, I would need to:

1. Create `guide-player-with-narrator-local.ts` (local version)
2. Modify Safety Net to call local LLM for primary AI
3. Keep API as fallback for safety AI (dual-backend approach)
4. Test extensively

**Benefits:**
- ğŸ–¥ï¸ Show `LOCAL` in logs
- ğŸ’° Reduced costs
- ğŸš€ Faster responses (no network)
- ğŸ“Š Track Local vs API usage

**Considerations:**
- Local LLM may not handle complex game master role as well
- Would need thorough testing
- Gemini Flash Lite is already very cheap (~$0.001 per command)

**Let me know if you want me to build this!** ğŸš€
