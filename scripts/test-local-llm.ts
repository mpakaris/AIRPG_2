/**
 * Test script to verify local LLM is working
 * Run with: npx tsx scripts/test-local-llm.ts
 */

// Load environment variables from .env file
import 'dotenv/config';

import { callLocalLLM, checkLocalLLMHealth } from '../src/ai/local-llm-client';
import { z } from 'zod';

const TestOutputSchema = z.object({
  commandToExecute: z.string(),
  responseToPlayer: z.string(),
});

async function testLocalLLM() {
  console.log('üîç Testing Local LLM Connection...\n');

  // Show configuration
  const baseUrl = process.env.LOCAL_LLM_BASE_URL || 'http://localhost:8080';
  const modelName = process.env.LOCAL_LLM_MODEL_NAME || 'llama3.2-3b';
  console.log('üìã Configuration:');
  console.log(`   Base URL: ${baseUrl}`);
  console.log(`   Model: ${modelName}\n`);

  // Test 1: Health Check
  console.log('1Ô∏è‚É£ Health Check...');
  const isHealthy = await checkLocalLLMHealth();

  if (!isHealthy) {
    console.log('‚ùå Local LLM is NOT responding');
    console.log('\nüí° Troubleshooting:');
    if (baseUrl.includes('11434')) {
      console.log('   ‚Ä¢ Check if Ollama is running: ps aux | grep ollama');
      console.log('   ‚Ä¢ Start Ollama: ollama serve &');
      console.log('   ‚Ä¢ Test Ollama: curl http://localhost:11434/api/tags');
    } else {
      console.log('   ‚Ä¢ Check if Docker container is running: docker ps | grep llm-server');
      console.log('   ‚Ä¢ Start container: docker start llm-server');
    }
    console.log('');
    process.exit(1);
  }

  console.log('‚úÖ Local LLM is healthy and responding\n');

  // Test 2: Simple Command Interpretation
  console.log('2Ô∏è‚É£ Testing Command Interpretation...');

  const systemPrompt = `You are a command interpreter for a text game.

You MUST respond with ONLY valid JSON in this exact format:
{"responseToPlayer": "message", "commandToExecute": "command"}

Available commands: examine, take, go, use

Example:
Input: "look at the door"
Output: {"responseToPlayer": "Looking at the door", "commandToExecute": "examine door"}

Remember: Output ONLY JSON, no other text!`;

  const userPrompt = 'Player input: "look at the door"\n\nRespond with ONLY JSON:';

  try {
    const startTime = Date.now();
    const result = await callLocalLLM(
      systemPrompt,
      userPrompt,
      TestOutputSchema
    );
    const duration = Date.now() - startTime;

    console.log('‚úÖ Successfully received response from local LLM');
    console.log(`‚è±Ô∏è  Response time: ${duration}ms`);
    console.log('\nüì¶ Result:');
    console.log(JSON.stringify(result, null, 2));
    console.log('\nüéâ Local LLM is working correctly!\n');

    // Test 3: Verify it's really local (no internet needed)
    console.log('3Ô∏è‚É£ Internet Check...');
    console.log('üí° The test succeeded, which means:');
    console.log(`   ‚úÖ Local LLM is running on ${baseUrl}`);
    console.log('   ‚úÖ Your Next.js app can communicate with it');
    console.log('   ‚úÖ Command interpretation will work offline');
    console.log('\nüî¨ To prove it\'s local, you can:');
    console.log('   1. Disconnect WiFi');
    console.log('   2. Run this test again (npm run test:llm)');
    console.log('   3. If it still works ‚Üí definitely using local LLM!\n');

  } catch (error) {
    console.log('‚ùå Failed to get response from local LLM');
    console.log('Error:', error);
    console.log('\nüí° Troubleshooting:');
    if (baseUrl.includes('11434')) {
      console.log('   ‚Ä¢ Check Ollama logs: tail -f ~/.ollama/logs/server.log');
      console.log('   ‚Ä¢ Test directly: ollama run llama3.2:3b "hello"');
    } else {
      console.log('   ‚Ä¢ Check Docker logs: docker logs llm-server');
    }
    console.log('');
    process.exit(1);
  }
}

testLocalLLM().catch(console.error);
